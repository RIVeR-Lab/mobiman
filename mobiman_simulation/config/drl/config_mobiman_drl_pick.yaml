#### DRL Parameters

flag_print_info: False

igibson_config_file: "igibson_jackal_jaco"

### Common Parameters for Training and Testing
ros_pkg_name: "mobiman_simulation"
gazebo_robot_name: "mobiman"
#drl_mode: "training"
drl_data_path: "dataset/drl/m4/"
#initial_training_path: ""
initial_training_path: "20240301_182134_SAC_mobiman/"
training_log_name: "training_log"

### DRL-Testing Parameters

#testing_log_name: "testing_log"
# m4
# ocs2_wb
testing_benchmark_name: "m4"
n_testing_eval_episodes: 1

### DRL-Training Parameters

## Algorithm:
# PPO
# SAC
# DDPG  (NUA NOTE: NOT FUNCTIONAL YET!)
# A2C   (NUA NOTE: NOT FUNCTIONAL YET!)
rl_algorithm: "SAC"
motion_planning_algorithm: "ocs2"

# Options: ### NUA TODO: UPDATE! 
# mobiman_FC             -> FC(occupancy_set + target + prev_action)
# mobiman_2DCNN_FC       -> 2DCNN(Channel: 1, Height: occupancy_set, Width: time) + FC(CNN_output + target + prev_action) (NUA NOTE: NOT FUNCTIONAL YET!)
observation_space_type: "mobiman_FC"

learning_rate: 0.00025
n_steps: 100 #1000
batch_size: 28 #50
ent_coef: 0.0002
training_timesteps: 30000 #10000
max_episode_steps: 10 #2000

training_checkpoint_freq: 50 #1000

plot_title: "Learning Curve"
plot_moving_average_window_size_timesteps: 5
plot_moving_average_window_size_episodes: 1

## Task:
# Options: 
# JackalJaco_mobiman_drl-v0
task_and_robot_environment_name: "JackalJaco_mobiman_drl-v00"   # (NUA NOTE: GAZEBO ONLY AND NOT FUNCTIONAL YET!)

world_name: "conveyor"

#### NUA TODO: SET THEM IN PYTHON SCRIPT! ---------------- START
world_range_x_min: -2.0               # wrt. world frame
world_range_x_max: 2.0                # wrt. world frame
world_range_y_min: -2.0               # wrt. world frame
world_range_y_max: 2.0                # wrt. world frame
world_range_z_min: 0.0                # wrt. world frame
world_range_z_max: 2.0                # wrt. world frame

init_robot_pos_range_x_min: -1.8      # wrt. world frame
init_robot_pos_range_x_max: 1.8       # wrt. world frame
init_robot_pos_range_y_min: -1.8      # wrt. world frame
init_robot_pos_range_y_max: 1.0       # wrt. world frame
#init_robot_pos_range_z_min: 0.0
#init_robot_pos_range_z_max: 2.0

goal_range_min_x: -0.8                # wrt. conveyor frame
goal_range_max_x: 0.8                 # wrt. conveyor frame
goal_range_min_y: -0.15               # wrt. conveyor frame
goal_range_max_y: 0.0                 # wrt. conveyor frame
goal_range_min_z: 0.6                 # wrt. world frame
goal_range_max_z: 0.7                 # wrt. world frame
#### NUA TODO: SET THEM IN PYTHON SCRIPT! ---------------- END

## Data and Sensors
world_frame_name: "world"
robot_frame_name: "base_link"
arm_joint_names: ["j2n6s300_joint_1", "j2n6s300_joint_2", "j2n6s300_joint_3", "j2n6s300_joint_4", "j2n6s300_joint_5", "j2n6s300_joint_6"]
ee_frame_name: "j2n6s300_end_effector"
goal_frame_name: "grasp"
occupancy_frame_names: ["conveyor_belt", "red_cube"]

odom_msg_name: "odom"
arm_state_msg_name: "joint_states"
base_control_msg_name: "mobile_base_controller/cmd_vel"
arm_control_msg_name: "arm_controller/cmd_pos"
target_msg_name: "target_visu"

mpc_data_msg_name: "mpc_data"
#mobiman_goal_obs_msg_name: "mobiman_goal_obs"
#mobiman_occupancy_obs_msg_name: "mobiman_occupancy_obs"
selfcoldistance_msg_name: "self_collision_distances"

self_collision_range_min: 0.0
self_collision_range_max: 1.0

#n_obs_stack: 1
#n_skip_obs_stack: 1

## Algorithm: mobiman-DRL Parameters
obs_base_velo_lat_min: -2.0
obs_base_velo_lat_max: 2.0
obs_base_velo_ang_min: -5.0
obs_base_velo_ang_max: 5.0

obs_joint_velo_min: -20.0
obs_joint_velo_max: 20.0

err_threshold_pos: 0.05
err_threshold_ori_yaw: 0.05
err_threshold_ori_quat: 0.05

# Action time horizon [s]
action_time_horizon: 6.0

# 0: Discrete
# 1: Continuous
action_type: 1

# [model(1), target_type(1), target(6)]
n_action: 8

action_goal_range_min_x: -0.0               # wrt. goal frame
action_goal_range_max_x: 0.0                # wrt. goal frame
action_goal_range_min_y: -0.0               # wrt. goal frame
action_goal_range_max_y: 0.0                # wrt. goal frame
action_goal_range_min_z: -0.0               # wrt. goal frame
action_goal_range_max_z: 0.0                # wrt. goal frame

action_target_range_min_x: -2.0             # wrt. world frame
action_target_range_max_x: 2.0              # wrt. world frame
action_target_range_min_y: -2.0             # wrt. world frame
action_target_range_max_y: 2.0              # wrt. world frame
action_target_range_min_z: 0.0              # wrt. world frame
action_target_range_max_z: 1.0              # wrt. world frame

## Reward Function:
# Terminal:
reward_terminal_goal: 100.0
reward_terminal_out_of_boundary: -100.0
reward_terminal_collision: -100.0
reward_terminal_rollover: -100.0
reward_terminal_max_step: -50.0

# Distance-to-goal
reward_step_dist2goal_scale: 10.0
reward_step_dist2goal_dist_threshold: 0.5
#reward_step_dist2goal_mu: 0.0
#reward_step_dist2goal_sigma: 1.0

# Model mode
reward_step_mode0: -6.0     # Computational Cost: 3
reward_step_mode1: -12.0    # Computational Cost: 6 
reward_step_mode2: -18.0    # Computational Cost: 9

# MPC result
#reward_step_mpc_exit: -5
#reward_step_target_reached: 3
#reward_step_time_horizon_min: -10
#reward_step_time_horizon_max: 0

# Cost weights
alpha_step_dist2goal: 1.0
alpha_step_mode: 1.0
#alpha_step_mpc_result: 1