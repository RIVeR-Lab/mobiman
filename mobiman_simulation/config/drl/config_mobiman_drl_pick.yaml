#### DRL Parameters

flag_print_info: False

igibson_config_file: "igibson_jackal_jaco"

### Common Parameters for Training and Testing
ros_pkg_name: "mobiman_simulation"
gazebo_robot_name: "mobiman"
#drl_mode: "training"

# m4
# ocs2wb
drl_data_path: "dataset/drl/m4/"      # NUA NOTE: Saving directory
#initial_training_path: ""
initial_training_path: "20240314_225038_PPO_mobiman/"
training_log_name: "training_log"

### DRL-Testing Parameters

#testing_log_name: "testing_log"
# m4
# ocs2wb
testing_benchmark_name: "m4"
n_testing_eval_episodes: 1

### DRL-Training Parameters

## Algorithm:
# PPO
# SAC
# DDPG
# A2C   (NUA NOTE: NOT FUNCTIONAL YET!)
rl_algorithm: "PPO"
motion_planning_algorithm: "ocs2"

# Options: ### NUA TODO: UPDATE! 
# mobiman_FC             -> FC(occupancy_set + target + prev_action)
# mobiman_2DCNN_FC       -> 2DCNN(Channel: 1, Height: occupancy_set, Width: time) + FC(CNN_output + target + prev_action) (NUA NOTE: NOT FUNCTIONAL YET!)
observation_space_type: "mobiman_FC"

learning_rate: 0.0002         #PPO: 0.0001    #SAC: 0.00025 
n_steps: 200                   #PPO: 100       #SAC: 100
batch_size: 50                #PPO: 20        #SAC: 28
ent_coef: 0.0001              #PPO: 0.0002    #SAC: 0.0002
training_timesteps: 50000
max_episode_steps: 20

training_checkpoint_freq: 50 #1000

plot_title: "Learning Curve"
plot_moving_average_window_size_timesteps: 5
plot_moving_average_window_size_episodes: 1

## Task:
# Options: 
# JackalJaco_mobiman_drl-v0
task_and_robot_environment_name: "JackalJaco_mobiman_drl-v00"   # (NUA NOTE: GAZEBO ONLY AND NOT FUNCTIONAL YET!)

world_name: "conveyor"

#### NUA TODO: SET THEM IN PYTHON SCRIPT! ---------------- START
world_range_x_min: -2.0               # wrt. world frame
world_range_x_max: 2.0                # wrt. world frame
world_range_y_min: -2.0               # wrt. world frame
world_range_y_max: 2.0                # wrt. world frame
world_range_z_min: 0.0                # wrt. world frame
world_range_z_max: 2.0                # wrt. world frame

init_robot_pos_range_x_min: -1.8      # wrt. world frame
init_robot_pos_range_x_max: 1.8       # wrt. world frame
init_robot_pos_range_y_min: -1.8      # wrt. world frame
init_robot_pos_range_y_max: 1.0       # wrt. world frame
#init_robot_pos_range_z_min: 0.0
#init_robot_pos_range_z_max: 2.0

goal_range_min_x: -0.8                # wrt. conveyor frame
goal_range_max_x: 0.8                 # wrt. conveyor frame
goal_range_min_y: -0.15               # wrt. conveyor frame
goal_range_max_y: 0.0                 # wrt. conveyor frame
goal_range_min_z: 0.6                 # wrt. world frame
goal_range_max_z: 0.7                 # wrt. world frame
#### NUA TODO: SET THEM IN PYTHON SCRIPT! ---------------- END

## Data and Sensors
world_frame_name: "world"
robot_frame_name: "base_link"
arm_joint_names: ["j2n6s300_joint_1", "j2n6s300_joint_2", "j2n6s300_joint_3", "j2n6s300_joint_4", "j2n6s300_joint_5", "j2n6s300_joint_6"]
ee_frame_name: "j2n6s300_end_effector"
goal_frame_name: "grasp"
occupancy_frame_names: ["conveyor_belt", "red_cube"]

odom_msg_name: "odom"
arm_state_msg_name: "joint_states"
base_control_msg_name: "mobile_base_controller/cmd_vel"
arm_control_msg_name: "arm_controller/cmd_pos"
target_msg_name: "target_visu"

mpc_data_msg_name: "mpc_data"
#mobiman_goal_obs_msg_name: "mobiman_goal_obs"
#mobiman_occupancy_obs_msg_name: "mobiman_occupancy_obs"
selfcoldistance_msg_name: "self_collision_distances"

self_collision_range_min: 0.0
self_collision_range_max: 1.0

#n_obs_stack: 1
#n_skip_obs_stack: 1

## Algorithm: mobiman-DRL Parameters
obs_base_velo_lat_min: -2.0
obs_base_velo_lat_max: 2.0
obs_base_velo_ang_min: -5.0
obs_base_velo_ang_max: 5.0

obs_joint_velo_min: -20.0
obs_joint_velo_max: 20.0

err_threshold_pos: 0.05
err_threshold_ori_yaw: 0.05
err_threshold_ori_quat: 0.05

# Action time horizon [s]
action_time_horizon: 3.0

# 0: Discrete
# 1: Continuous [model(1), target_type(1), target(6)]
action_type: 0

action_discrete_trajectory_data_path: ["dataset/trajectory_sampling/jackal/20240312_153905/", 
                                       "dataset/trajectory_sampling/jaco/20240313_202938/",
                                       "dataset/trajectory_sampling/jackalJaco/20240312_143237/"]

action_goal_range_min_x: -0.0               # wrt. goal frame
action_goal_range_max_x: 0.0                # wrt. goal frame
action_goal_range_min_y: -0.0               # wrt. goal frame
action_goal_range_max_y: 0.0                # wrt. goal frame
action_goal_range_min_z: -0.0               # wrt. goal frame
action_goal_range_max_z: 0.0                # wrt. goal frame

action_target_range_min_x: -1.0             # wrt. world frame
action_target_range_max_x: 1.0              # wrt. world frame
action_target_range_min_y: -1.0             # wrt. world frame
action_target_range_max_y: 1.0              # wrt. world frame
action_target_range_min_z: 0.0              # wrt. world frame
action_target_range_max_z: 1.0              # wrt. world frame

## Reward Function:
# Terminal:
reward_terminal_goal: 100.0
reward_terminal_out_of_boundary: -100.0
reward_terminal_collision: -100.0
reward_terminal_rollover: -100.0
reward_terminal_max_step: -50.0

# Model mode
reward_step_mode0: -3.0     # Computational Cost: 3
reward_step_mode1: -6.0    # Computational Cost: 6 
reward_step_mode2: -9.0    # Computational Cost: 9

# Goal
reward_step_dist2goal_scale: 10.0
reward_step_dist2goal_dist_scale: 0.2
reward_step_dist2goal_dist_threshold: 1.0

# Target
reward_step_target_scale: 10.0
reward_step_target_intermediate_point_scale: 0.5
reward_step_target_gamma: -4.0

# Cost weights
alpha_step_mode: 1.0
alpha_step_goal: 1.0
alpha_step_target: 1.0